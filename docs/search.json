[
  {
    "objectID": "wikipedia.html",
    "href": "wikipedia.html",
    "title": "Wikipedia Module",
    "section": "",
    "text": "from seventh import wikipedia\n\n\n\nPage\n\n Page (title:str, extract:str)\n\nA Pydantic Dataclass Class that represents a page from wikipedia\n\n\n\nrandom_page\n\n random_page (language:str='en')\n\nGet a random page from Wikipedia.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlanguage\nstr\nen\nThe language edition of wikipedia\n\n\nReturns\nPage\n\nA wikipedia page object\n\n\n\nIn this function we connect to https://{language}.wikipedia.org/api/rest_v1/page/random/summary, and extract the result from the JSON sent back, using pydantic. You can tey it with English Here and German Here\n\nwikipedia.random_page()\n\nPage(title='El Salvador–United States relations', extract=\"According to the 2012 U.S. Global Leadership Report, 55% of Salvadorans approve of U.S. leadership, with 19% disapproving and 26% uncertain, the fourth-highest rating for any surveyed country in the Americas. In 2013 and 2014, according to the Pew Research Center's global attitudes survey 79% and 80% of Salvadorans viewed the United States positively respectively revealing El Salvador as one of the most pro-American nations in the world.\")\n\n\n\nwikipedia.random_page(language=\"de\")\n\nPage(title='Hans-Rolf Tränkler', extract='Hans-Rolf Tränkler ist ein deutscher Ingenieur und Professor für Messtechnik und Sensorik. Er gehört zu den maßgeblichen Professoren, die das Fach Messtechnik und Sensorik in Lehre und Forschung geprägt haben.')",
    "crumbs": [
      "Wikipedia Module"
    ]
  },
  {
    "objectID": "howmade.html",
    "href": "howmade.html",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "",
    "text": "This is a riff on Claudio JoloWicz’s Hyper Modern Python done in a way which:",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#a-simple-command-line-application",
    "href": "howmade.html#a-simple-command-line-application",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "A simple command line application",
    "text": "A simple command line application\npixi global install direnv\nTo your .zshrc add: eval \"$(direnv hook zsh)\"\nAnd then do:\nsource .zshrc or start a new shell…\nThen lets create our project.\npixi init First --pyproject\ncd First\npixi add python jupyterlab ipykernel pixi-kernel click\npixi creates editable installs by default if you are using --pyproject. This option is recommended for python packages. For complex packages they say to use pixi.toml.\nI feel the distinction comes from whether you are going to build or not into a library. But for any ml project you want a editable install. You'll have to get this working in ci/cd as well.\nLets create the appropriate folders:\nmkdir first; touch first/__init__.py\nIn __init__.py we add:\nimport importlib.metadata\n\n__version__ = importlib.metadata.version(__name__ or __package__)\nand in first/console.py we add:\nimport click\nfrom first import __version__\nimport requests\nimport textwrap\n\nAPI_URL = \"https://en.wikipedia.org/api/rest_v1/page/random/summary\"\n\n@click.command()\n@click.version_option(version=__version__)\ndef main():\n    \"\"\"The ultramodern Python project.\"\"\"\n    with requests.get(API_URL) as response:\n        response.raise_for_status()\n        data = response.json()\n        title = data[\"title\"]\n        extract = data[\"extract\"]\n        click.secho(title, fg=\"green\")\n        click.echo(textwrap.fill(extract))\n\nif __name__==\"__main__\":\n    main()\nNow lets add this file as a task in pixi:\npixi task add first \"python first/console.py\"\nnow you can do:\npixi run first\npixi run first --version\nThe latter should return 0.1.0",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#testing",
    "href": "howmade.html#testing",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Testing",
    "text": "Testing\nThe first thing we do is to create a copy of first called second`.\nNow in normal situations you would create a testing environment. pixi is quite general, so what we first do is create a testing feature instead, and fold that into a testing environment. In the future we can fold other stuff “features” into this testing environment. For example, we can create features for formatting and linting, and fold that all into a dev environment. Ditto for CI.\npixi add --feature test pytest\nSo we added the pytest package to the feature test. Now we’ll make a test file to run tests:\nmkdir tests\ntouch tests/test_console.py\nand in test_console.py we do:\n# tests/test_console.py\nimport click.testing\nfrom second import console\ndef test_main_succeeds():\n    runner = click.testing.CliRunner()\n    result = runner.invoke(console.main)\n    assert result.exit_code == 0\nNow it is time to add the test environment. In pyproject.toml we do:\n# Environments\n[tool.pixi.environments]\ndefault = { solve-group = \"default\" }\ntest = { features = [\"test\"], solve-group = \"default\" }\nso we create a test environment which augments the default. In addition there is this interesting notion of something called a solve-group. What is this?\nFor environments we could do:\n[environments]\n# implicit: default = [\"default\"]\ndefault = [\"py39\"] # implicit: default = [\"py39\", \"default\"]\npy310 = [\"py310\"] # implicit: py310 = [\"py310\", \"default\"]\ntest = [\"test\"] # implicit: test = [\"test\", \"default\"]\ntest39 = [\"test\", \"py39\"] # implicit: test39 = [\"test\", \"py39\", \"default\"]\nThe solve group is used to group environments together at the solve stage. This is useful for environments that need to have the same dependencies but might extend them with additional dependencies. For instance when testing a production environment with additional test dependencies.\n[environments]\n# Creating a `prod` environment which is the minimal set of dependencies used for production.\nprod = {features = [\"py39\"], solve-group = \"prod\"}\n# Creating a `test_prod` environment which is the `prod` environment plus the `test` feature.\ntest_prod = {features = [\"py39\", \"test\"], solve-group = \"prod\"}\n# Using the `solve-group` to solve the `prod` and `test_prod` environments together\n# Which makes sure the tested environment has the same version of the dependencies as the production environment.\nNow you can run stuff in the test environment with pixi run -e test python.\nThis enables us to create a task:\n[tool.pixi.feature.test.tasks]\ntest = \"pytest\"\nand then doing pixi run test just does the right thing. It figures out that the test task calling pytest is available in the test environment and runs in that.\nIf the environment cannot be uniquely resolved, you will be offered a choice…\nNow since the clirunner may need to be used for multiple tests we can create it as a pytest fixture, and use it in multiple tests.\n# tests/test_console.py\nimport click.testing\nimport pytest\nfrom second import console\n\n@pytest.fixture\ndef runner():\n    return click.testing.CliRunner()\n\ndef test_main_succeeds(runner):\n    result = runner.invoke(console.main)\n    assert result.exit_code == 0\n\nAdding coverage\nAdding coverage is simple.\npixi add --feature test pytest-cov\nThen because we need optional dependencies and conda does not support those:\npixi add --feature test --pypi \"coverage[toml]\"\nNow all you need to do if pixi run test --cov which gets ececuted in the test environment with the test feature and gives you coverage output.\nYou can configure Coverage.py to require full test coverage (or any other target percentage) using the fail_under option:\n[tool.coverage.report]  \nfail_under = 100\n\n\nMocking\nWe start by pixi add --feature test pytest-mock\nUnit tests should be fast, isolated, and repeatable. The test for console.mainis neither of these:\n\nIt is not fast, because it takes a full round-trip to the Wikipedia API to complete.\nIt does not run in an isolated environment, because it sends out an actual request over the network.\nIt is not repeatable, because its outcome depends on the health, reachability, and behavior of the API. In particular, the test fails whenever the network is down.\n\nNow we can add a whole bunch of tests that show off the mock behavior. Notice how mock is returned as a fixture. It patched requests.get as a context manager:\ndef main():\n    \"\"\"The ultramodern Python project.\"\"\"\n    with requests.get(API_URL) as response:\n        response.raise_for_status()\n        data = response.json()\n        title = data[\"title\"]\n        extract = data[\"extract\"]\n        click.secho(title, fg=\"green\")\n        click.echo(textwrap.fill(extract))\nby fake-calling the __enter__ and json and mucking with their return values, and then we test against these fake values.\n@pytest.fixture\ndef runner():\n    return click.testing.CliRunner()\n\n# def test_main_succeeds(runner):\n#     result = runner.invoke(console.main)\n#     assert result.exit_code == 0\n\n@pytest.fixture\ndef mock_requests_get(mocker):\n    mock = mocker.patch(\"requests.get\")\n    mock.return_value.__enter__.return_value.json.return_value = {\n            \"title\": \"Lorem Ipsum\",\n            \"extract\": \"Lorem ipsum dolor sit amet\",\n    }\n    return mock\n\ndef test_main_succeeds(runner, mock_requests_get):\n    result = runner.invoke(console.main)\n    assert result.exit_code == 0\n\ndef test_main_prints_title(runner, mock_requests_get):\n    result = runner.invoke(console.main)\n    assert \"Lorem Ipsum\" in result.output\n\ndef test_main_invokes_requests_get(runner, mock_requests_get):\n    runner.invoke(console.main)\n    assert mock_requests_get.called\n\ndef test_main_uses_correct_url(runner, mock_requests_get):\n    runner.invoke(console.main)\n    assert mock_requests_get.call_args == ((console.API_URL,),)\n\ndef test_main_fails_on_request_error(runner, mock_requests_get):\n    mock_requests_get.side_effect = Exception(\"Boom\")\n    result = runner.invoke(console.main)\n    assert result.exit_code == 1\n\n\nRefactoring using tests\nWe can use the tests we have created to re-factor our code.\nWe create a wikipedia.py:\nimport requests  \nAPI_URL = \"https://en.wikipedia.org/api/rest_v1/page/random/summary\"  \ndef random_page():  \n    with requests.get(API_URL) as response:  \n        response.raise_for_status()  \n        return response.json()\nand change the call in console.py:\ndef main():  \n    \"\"\"The hypermodern Python project.\"\"\"  \n    data = wikipedia.random_page()    title = data[\"title\"]  \n    extract = data[\"extract\"]    click.secho(title, fg=\"green\")  \n    click.echo(textwrap.fill(extract))\nAll the tests continue to run, because we have continued to mock requests.get.\nNow let us handle exceptions gracefully…\nimport requests  \ndef test_main_prints_message_on_request_error(runner, mock_requests_get):  \n    mock_requests_get.side_effect = requests.RequestException  \n    result = runner.invoke(console.main)  \n    assert \"Error\" in result.output\nThe test now fails. The test can be fixed with:\ndef random_page():\n    try:\n        with requests.get(API_URL) as response:\n            response.raise_for_status()\n            return response.json()\n    except requests.RequestException as error:\n        message = str(error)\n        raise click.ClickException(message)\nIn the test we make sure we raise a RequestException; this is what happens if the internet breaks in-between client and server. However that does not lead to our test passing , because the result’s output is not affected. We make this happen by making a ClickException with a stringified error message.\nNow lets add an option to choose the language..we start by writing a test in a new file test_wikipedia.py, and move the mock_requests_get fixture into conftests.py so it can be made available to anything…\nfrom third import wikipedia\n\ndef test_random_page_uses_given_language(mock_requests_get):\n    wikipedia.random_page(language=\"de\")\n    args, _ = mock_requests_get.call_args\n    assert \"de.wikipedia.org\" in args[0]\nNow we need to fix random_page as we get TypeError: random_page() got an unexpected keyword argument 'language'…\ndef random_page(language=\"en\"):\n    try:\n        with requests.get(API_URL.format(language=language)) as response:\n            response.raise_for_status()\n            return response.json()\n    except requests.RequestException as error:\n        message = str(error)\n        raise click.ClickException(message)\nWe’ll add a command-line option to choose the language now…we add a test…\n@pytest.fixture  \ndef mock_wikipedia_random_page(mocker):  \n    return mocker.patch(\"hypermodern_python.wikipedia.random_page\")\ndef test_main_uses_specified_language(runner, mock_wikipedia_random_page):  \n    runner.invoke(console.main, [\"--language=pl\"])  \n    mock_wikipedia_random_page.assert_called_with(language=\"pl\")\nNow we add the click option:\n@click.command()\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=\"en\",\n    help=\"Language edition of Wikipedia\",\n    metavar=\"LANG\",\n    show_default=True,\n)\n@click.version_option(version=__version__)\ndef main(language):\n    \"\"\"The ultramodern Python project.\"\"\"\n    data = wikipedia.random_page(language=language)\n    title = data[\"title\"]\n    extract = data[\"extract\"]\n    click.secho(title, fg=\"green\")\n    click.echo(textwrap.fill(extract))\nNow its all working\n\n\nEnd to End Testing\nOk we’d like to bring back the integration-like tests where we actually hit wikipedia instead of mocking it out, and using the mocks for TDD.",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#test-automation",
    "href": "howmade.html#test-automation",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Test automation",
    "text": "Test automation\nAs a precursor for testing against various pythons, and then further under various operating systems, as github actions, we’d want to set up python environments corresponding to different versions of python. Back in the day, we;d use tox or nox to do this, but now this goodness is given to us by pixi, by its various features and environments.\nI had to make quite a few changes from the simple additions that i had earlier to get this to work. In particular, I had to loosen pixi’s default restrictions on python versions being &gt;= 3.11. I also had to make python=* in the default although i am not sure that was needed if the above restriction was removed. I also mistakenly used ~= dependencies at the top level which bumped up everything to 3.12.\nNow the pyproject.toml looks like this…\n[project]\nname = \"fourth\"\nversion = \"0.1.0\"\ndescription = \"Add a short description here\"\nauthors = [{ name = \"Rahul Dave\", email = \"rahuldave@gmail.com\" }]\nrequires-python = \"&gt;= 3.10\"\ndependencies = []\n\n[project.optional-dependencies]\ntest = [\"coverage[toml]\"]\n\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.pixi.project]\nchannels = [\"conda-forge\"]\nplatforms = [\"osx-arm64\"]\n\n[tool.pixi.tasks]\nfourth = \"python fourth/console.py\"\n\n[tool.pixi.dependencies]\npixi-kernel = \"&gt;=0.3.0,&lt;0.4\"\njupyterlab = \"&gt;=4.2.0,&lt;4.3\"\npython = \"*\"\nipykernel = \"&gt;=6.29.3,&lt;6.30\"\nclick = \"&gt;=8.1.7,&lt;8.2\"\n\n[tool.pixi.pypi-dependencies]\nfourth = { path = \".\", editable = true }\n\n[tool.pixi.feature.test.dependencies]\npytest = \"&gt;=7.2.0,&lt;8.3\"\npytest-cov = \"&gt;=5.0.0,&lt;5.1\"\npytest-mock = \"&gt;=3.14.0,&lt;3.15\"\n\n[tool.pixi.feature.test.tasks]\ntest = \"pytest --cov -m 'not e2e'\"\nteste2e = \"pytest --cov -m 'e2e'\"\n\n[tool.pixi.feature.py310.dependencies]\npython = \"3.10.*\"\n\n[tool.pixi.feature.py311.dependencies]\npython = \"3.11.*\"\n\n# Environments\n[tool.pixi.environments]\ndefault = { solve-group = \"default\" }\ntest = { features = [\"test\"], solve-group = \"default\" }\npy310 = { features = [\"py310\"], solve-group = \"py310\" }\npy311 = { features = [\"py311\"], solve-group = \"py311\" }\npy310test = { features = [\"py310\", \"test\"], solve-group = \"py310\" }\npy311test = { features = [\"py311\", \"test\"], solve-group = \"py311\" }\n\n[tool.coverage.paths]\nsource = [\"fourth\"]\n[tool.coverage.run]\nbranch = true\nsource = [\"fourth\"]\n[tool.coverage.report]\nshow_missing = true\nand now i can train on a matrix of test, py310test, and py311test. One could drop the py310 and py311 environments if one was pnly using them for testing. Notice how i set up different solve groups as well. I am not sure i need this but it ensures that if an older version of python cannot be so;ved into the default group it can construct a new group for it. I need to ask about this.\nIn nox you can run multiple tests (sessions) together. I do not know what the equivalent for pixi is.\nNow we can push this up a notch by having a CI/CD pipeline.\n\nCI/CD\nContinuous integration (CI) helps you automate the integration of code changes into your project. When changes are pushed to the project repository, the CI server verifies their correctness, triggering tools such as unit tests, linters, or type checkers.\nPull Requests are an important building block in this workflow. They let you propose a set of changes to the repository, for example a specific bugfix or new feature. When the pull request gets accepted, its changes are merged into the target branch, typically master. GitHub displays Pull Requests with a green tick if they pass CI, and with a red x if the CI pipeline failed. In this way, continuous integration functions as a gate commits need to pass to enter the master branch.\nWe’ll take baby steps towards this by setting up our testing on github actions. Later, we’ll create a more complex matrix of tests, including different platforms, and including formatting and linting. All of this will make sure that the contributions are in the format we desire..\nWe create a folder .github/workflows and creat a file test.yaml in there:\nname: CI\n\non:\n  pull_request:\n  push:\n    branches:\n      - main\n\njobs:\n  tests-per-env:\n    runs-on: macos-latest\n    strategy:\n      matrix:\n        environment: [test, py310test, py311test]\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v4\n      - name: Setup pixi\n        uses: prefix-dev/setup-pixi@v0.6.0\n        with:\n          environments: ${{ matrix.environment }}\n      - name: Test with Pixi\n        run: |\n          pixi run --environment ${{ matrix.environment }} test\nthis workflow will run in github actions on pushes and pull requests, basically running the test task in multiple test envoronments..",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#lint-and-format",
    "href": "howmade.html#lint-and-format",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Lint and Format",
    "text": "Lint and Format\nIn the old days we’d use black and flake8. Now there is a faster option, ruff which makes things faster in local and ci/cd. To do this we’ll pixi add ruff --feature lintformat.\n\nLinting\nif you just do `ruff check you get:\n❯ ruff check tests\ntests/test_console.py:39:1: E402 Module level import not at top of file\nFound 1 error.\n\nfifth on  main [?] is 📦 v0.1.0 via 🐍 v3.12.3 via 🅒 fifth\n❯ ruff check fifth\nAll checks passed!\nThis is good. Lets fix the erroe in the tests folder by moving import requests to the top of the file.\nIn the original post by Claudio, the FECW flags are chosen. Lets do that in ruff.\n\nF are errors reported by pyflakes, a tool which parses source files and finds invalid Python code.\nW and E are warnings and errors reported by pycodestyle, which checks your Python code against some of the style conventions in PEP 8.\nC are violations reported by mccabe, which checks the code complexity of your Python package against a configured limit.\n\nIn ruff we set that up as:\n[tool.ruff.lint]\nselect = [\n    # pycodestyle\n    \"E\",\n    \"W\",\n    # Pyflakes\n    \"F\",\n    # McCabe\n    \"C\"\n]\nmccabe.max-complexity = 10\nWe hget two warnings now:\n❯ ruff check\ntests/test_console.py:50:1: W191 Indentation contains tabs\ntests/test_console.py:51:1: W191 Indentation contains tabs\nFound 2 errors.\nLets fix these.\nNow we can try to do import sorting by adding \"I\" in `select`:\nfifth on  main [?] is 📦 v0.1.0 via 🐍 v3.12.3 via 🅒 fifth\n❯ ruff check\nfifth/console.py:1:1: I001 [*] Import block is un-sorted or un-formatted\nfifth/wikipedia.py:1:1: I001 [*] Import block is un-sorted or un-formatted\ntests/test_console.py:2:1: I001 [*] Import block is un-sorted or un-formatted\nFound 3 errors.\n[*] 3 fixable with the `--fix` option.\nWe can let it fix the errors for us!\nNow we add flake-bugbear compatability which gives us all kinds of good ideas.\n❯ ruff check\nfifth/wikipedia.py:15:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\nFound 1 error.\nAnd we fix this via raise click.ClickException(message) from error\nNow we add flake8-bandit, but exclude callas to assert (how to do this on a directory basis in ruff?)\n❯ ruff check\nfifth/wikipedia.py:10:14: S113 Probable use of requests call without timeout\nFound 1 error.\nWe add a timeout in the requests call to fix…\n\n\nCode Formatting\nThe ruff formatter has been designed as a drop-in replacement for black.\nruff format\n\n\nPutting it into the tasks system\nWe added a feature:\n[tool.pixi.feature.lintformat.dependencies]\nruff = \"&gt;=0.4.4,&lt;0.5\"\nand some tasks for that feature:\n[tool.pixi.feature.lintformat.tasks]\nlint = \"ruff check\"\nformat = \"ruff format\"\nand now we add in the environments:\n[tool.pixi.environments]\ndefault = { solve-group = \"default\" }\ntest = { features = [\"test\"], solve-group = \"default\" }\ndev = { features = [\"test\", \"lintformat\"], solve-group = \"default\" }\npy310 = { features = [\"py310\"], solve-group = \"py310\" }\npy311 = { features = [\"py311\"], solve-group = \"py311\" }\npy310dev = { features = [\"py310\", \"test\", \"lintformat\"], solve-group = \"py310\" }\npy311dev = { features = [\"py311\", \"test\", \"lintformat\"], solve-group = \"py311\" }\npy310test = { features = [\"py310\", \"test\"], solve-group = \"py310\" }\npy311test = { features = [\"py311\", \"test\"], solve-group = \"py311\" }",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#pre-commit",
    "href": "howmade.html#pre-commit",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Pre-commit",
    "text": "Pre-commit\nYou want formatting to run before you commit. This can be run as a pre-commit hook in git. There is a nice piece of software, pre-commit which allows you to create these hooks.\nI’ll install it in the lintformat feature thus:\npixi add pre-commit --feature=lintformat\nAfter doing this we need to add a .pre-commit-config.yaml in the root folder of the project:\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v2.3.0\n    hooks:\n      - id: check-yaml\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n  - repo: local\n    hooks:\n      - id: ruff-format\n        name: ruff-format\n        entry: pixi run --environment dev format\n        language: system\n        types: [python]\nI’ll use our local python 3.12 dev environment to do the formatting.\nThen run pre-commit install. This will install the hook. You are done. On the next commit, pre-commit will run all the hooks listed above and then open your editor to write your commit message.",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#python-typing",
    "href": "howmade.html#python-typing",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Python Typing",
    "text": "Python Typing\nPython has non-compulsory typing which can nevertheless catch a lot of errors. You can use a typechecker for this purpose. We’ll use mypy. Both Visual Studio Code and Zed also use pyright and pylance so we enable the use of the current pixi shell which i set in the environment dev to use these (the env is set up as pixi shell --environment dev):\n[tool.pyright]\nvenvPath = \".\"\nvenv = \".pixi/envs/dev\"\nI install mypy in a new feature: pixi add mypy --feature typing. I then include this feature in all the dev environments, for eg.:\ndev = { features = [\"test\", \"lintformat\", \"typing\"], solve-group = \"default\" }\nand then configure mypy:\n[tool.mypy]\nwarn_return_any = true\nwarn_unused_configs = true\n\n[[tool.mypy.overrides]]\nmodule = [\"requests\", \"pytest\"]\nignore_missing_imports = true\nI also have the linter (ruff lint) add support for types (flake8-annotations):\n[tool.ruff.lint]\nignore = [\n    \"E203\",   # whitespace before ':'\n    \"S101\",   # use of assert detected, ignore for test-suite\n    \"ANN401\",\n]\nselect = [\n    # pycodestyle\n    \"E\",\n    \"W\",\n    # Pyflakes\n    \"F\",\n    # McCabe\n    \"C\",\n    # # pyupgrade\n    # \"UP\",\n    # # flake8-bugbear\n    \"B\",\n    # flake8-bandit\n    \"S\",\n    # # flake8-simplify\n    # \"SIM\",\n    # isort\n    \"I\",\n    \"ANN\",\n]\nmccabe.max-complexity = 10\n\n[tool.ruff.lint.flake8-annotations]\nallow-star-arg-any = true\nignore-fully-untyped = true\nNow ruff lint will look for types not supplied as well.\nNow we add types all over the place, for example:\ndef random_page(language: str = \"en\") -&gt; Any: ...\nFrom console.py:\ndef main(language: str = \"en\") -&gt; None:\n    \"\"\"The ultramodern Python project.\"\"\"\n    page = wikipedia.random_page(language=language)  # language=language needed here\n    # title = data[\"title\"]\n    # extract = data[\"extract\"]\n    click.secho(page.title, fg=\"green\")\n    click.echo(textwrap.fill(page.extract))\nand a part of test_console.py for example:\n# tests/test_console.py\nfrom unittest.mock import Mock\n\nimport pytest\nimport requests\nfrom click.testing import CliRunner\nfrom pytest_mock import MockFixture\n\nfrom sixth import console, wikipedia\n\n\n@pytest.fixture\ndef runner() -&gt; CliRunner:\n    return CliRunner()\n\n@pytest.mark.e2e\ndef test_main_succeeds_in_production_env(runner: CliRunner) -&gt; None:\n    result = runner.invoke(console.main)\n    assert result.exit_code == 0\n\n\ndef test_main_succeeds(runner: CliRunner, mock_requests_get: Mock) -&gt; None:\n    result = runner.invoke(console.main)\n    assert result.exit_code == 0\n\n...\n\n@pytest.fixture\ndef mock_wikipedia_random_page(mocker: MockFixture) -&gt; Mock:\n    return mocker.patch(\"sixth.wikipedia.random_page\")\n\n\ndef test_main_uses_specified_language(\n    runner: CliRunner, mock_wikipedia_random_page: Mock\n) -&gt; None:\n    runner.invoke(console.main, [\"--language=pl\"])\n    mock_wikipedia_random_page.assert_called_with(language=\"pl\")\nor in the test configurator conftest.py:\nfrom typing import Any\nfrom unittest.mock import Mock\n\nimport pytest\nfrom pytest_mock import MockFixture\n\n\ndef pytest_configure(config: Any) -&gt; None:\n    config.addinivalue_line(\"markers\", \"e2e: mark as end-to-end test.\")\n\n\n@pytest.fixture\ndef mock_requests_get(mocker: MockFixture) -&gt; Mock:\n    mock = mocker.patch(\"requests.get\")\n    mock.return_value.__enter__.return_value.json.return_value = {\n        \"title\": \"Lorem Ipsum\",\n        \"extract\": \"Lorem ipsum dolor sit amet\",\n    }\n    return mock\n\nReplace Anys with pydantic\nAny is a good catch-all when you dont know how to type. One thing one would like to do with typing is to get type validation. Pydantic is a great source for this.\nIn this case Any is not satisfactory. We want a type representing this wikipedia page. so after doing pixi add pydantic, we define a type for the page in wikipedia.py:\nimport click\nimport requests\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Page(BaseModel):\n    title: str\n    extract: str\n\n\n# API_URL = \"https://en.wikipedia.org/api/rest_v1/page/random/summary\"\nAPI_URL: str = \"https://{language}.wikipedia.org/api/rest_v1/page/random/summary\"\n\n\ndef random_page(\n    language: str = \"en\",\n) -&gt; Page:  # this is not a guaranteed keyword argument\n    try:\n        with requests.get(API_URL.format(language=language), timeout=10) as response:\n            response.raise_for_status()\n            data = response.json()\n            return Page(**data)\n    except (requests.RequestException, ValidationError) as error:\n        message: str = str(error)\n        raise click.ClickException(message) from error\nThe Page(**data) will construct a page object is the keys title and extract match in the json, else throw a ValidationError, which we now catch and re-throw. We add a test to deal with this, and a e2e test as well:\n@pytest.mark.e2e\ndef test_random_page_returns_page(mock_requests_get: Mock) -&gt; None:\n    page = wikipedia.random_page()\n    assert isinstance(page, wikipedia.Page)\n\n\ndef test_random_page_handles_validation_errors(mock_requests_get: Mock) -&gt; None:\n    mock_requests_get.return_value.__enter__.return_value.json.return_value = {}\n    with pytest.raises(click.ClickException): # title and excerpt not matched\n        wikipedia.random_page()\nAt this point run mypy ., ruff lint to catch any typing errors.\nWe add a typing task in the dev environments:\n[tool.pixi.feature.typing.tasks]\ntyping = \"mypy .\"",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "howmade.html#documentation",
    "href": "howmade.html#documentation",
    "title": "How this was made: Ultra Modern and Simple Python",
    "section": "Documentation",
    "text": "Documentation\nThere are many ways to document python. The old fashioned ways were via sphinx and read-the-docs: we’re going to use nbdev/notebooks/qmd files and quarto, which allows us rich media documentation. One of our key goals is that documentation is not just automatically generated, but that thought is put into the documentation and many examples are provided, WITHOUT compromising the documentation inlaid with code and without making it too long. We also want to solve the DRY problem with python documentation where parameters are repeated in docstrings: for this we use “docments” from fastcore and nbdev. We also want to make sure to be able to run our documentation against the lastest code, ie the code we have in our dev environments when we generate it…documentation should never be out of sync.\nSo, to summarize, we want:\n\nAutomatically generated but thoughtful documentation, with not overly long code files.\nDRY documentation, which is run and thus in sync with current code\nRich media documentation using notebooks and markdown files, which allows for high-quality html and pdf documentation using tools such as quarto and pandoc.\n\nDocumentation for nbdev is here.\nWe do:\npixi add jupyterlab quarto --feature docs\nI landed up having to get nbdev from pypi because the conda-forge channel has a very old version:\npixi add --pypi nbdev --feature docs\nThen I add the docs feature to all the dev environments.\nWe’ll also create a package = ['seventh'] so when we create a folder notebooks where we keep our testing, trying, documenting notebooks, so that it does not mess setuptools up.",
    "crumbs": [
      "How this was made: Ultra Modern and Simple Python"
    ]
  },
  {
    "objectID": "console.html",
    "href": "console.html",
    "title": "Console Module",
    "section": "",
    "text": "from seventh import console\n\nConsole really does not export any functions, but provides the command line API of the program.\n\n\n\n\n &lt;Command main&gt; (*args:Any, **kwargs:Any)\n\nThe ultramodern Python project.\nYou cant call this function directly, it becomes a click API. You must call it from the command line\n\n!python ../seventh/console.py --language pl\n\nKluczkowice-Osiedle\nKluczkowice-Osiedle – wieś w Polsce położona w województwie lubelskim,\nw powiecie opolskim, w gminie Opole Lubelskie.\n\n\nOr you can call it using pixi\n\n!pixi run program --language fr\n\n✨ Pixi task (program in default): python seventh/console.py --language fr\n⠁ activating environment                                                                 Lucgarier\nLucgarier est une commune française située dans le département des\nPyrénées-Atlantiques, en région Nouvelle-Aquitaine.\n\n\nTODO: make this a commanf line application",
    "crumbs": [
      "Console Module"
    ]
  }
]
